{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62447b94-fce8-4c52-93f1-71f750d957b4",
   "metadata": {},
   "source": [
    "# Diabetes Risk Prediction\n",
    "This project uses the Behavioral Risk Factor Surveillance System (BRFSS) survey data from [this link](https://www.cdc.gov/brfss/annual_data/annual_2024.html) to predict the probability of developing different types of Diabetes. Features about U.S. residents include demographic data (e.g. income level, education, race) as well as data regarding health-related risk behaviors, chronic health conditions, and use of preventive services.\n",
    "\n",
    "This is the first notebook for the project, which parses the raw ASCII data file, available in the link above, to extract the relevant target and feature variables for subsequent EDA and modeling in another notebook in this folder.\n",
    "\n",
    "The dataset contains 2 identifier columns, 3 Target variable candidates and a total of 24 potential features, as described below:\n",
    "- Each row is defined by a uniquely defined by (i.e. Table's Grain)\n",
    "  1. \"State FIPS Code\"\n",
    "  2. \"Annual Sequence Number\" \n",
    "- Target variable candidates related to Diabetes:\n",
    "  1. \"(Ever told) you had diabetes\"\n",
    "  2. \"Ever been told by a doctor or other health professional that you have pre-diabetes or borderline diabetes?\",\n",
    "  3. \"What type of diabetes do you have?\"\n",
    "- Demographic features:\n",
    "  1. \"Urban/Rural Status\"\n",
    "  2. \"Reported age in five-year age categories calculated variable\"\n",
    "  3. \"Sex of Respondent\"\n",
    "  4. \"Computed Race-Ethnicity grouping\"\n",
    "  5. \"Education Level\"\n",
    "  6. \"Income Level\"\n",
    "- Personal health features:\n",
    "  1. \"Have Personal Health Care Provider?\"\n",
    "  2. \"Could Not Afford To See Doctor\"\n",
    "  3. \"Computed Weight in Kilograms\"\n",
    "  4. \"Computed Height in Meters\"\n",
    "  5. \"Computed body mass index\"\n",
    "  6. \"Exercise in Past 30 Days\"\n",
    "  7. \"How often did you drink regular soda or pop that contains sugar?\"\n",
    "  8. \"How often did you drink sugar-sweetened drinks?\"\n",
    "  9. \"Computed Smoking Status\"\n",
    "  10. \"Computed number of drinks of alcohol beverages per week\"\n",
    "  11. \"Drink any alcoholic beverages in past 30 days\"\n",
    "  12. \"Heavy Alcohol Consumption  Calculated Variable\"\n",
    "  13. \"General Health\"\n",
    "- Other disease indicator features:\n",
    "  1. \"Ever Diagnosed with Heart Attack\"\n",
    "  2. \"Ever Diagnosed with Angina or Coronary Heart Disease\"\n",
    "  3. \"Ever Diagnosed with a Stroke\"\n",
    "  4. \"Ever told you have kidney disease?\"\n",
    "  5. \"Ever Told Had Asthma\"\n",
    "  6. \"(Ever told) you had a depressive disorder\"\n",
    "  7. \"Told Had Arthritis\"\n",
    "\n",
    "## Setup\n",
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3749e1-326d-4612-849d-9d14c26a973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URLs with input data\n",
    "raw_data_url = \"https://www.cdc.gov/brfss/annual_data/2024/files/LLCP2024ASC.zip\"\n",
    "data_dict_url = \"https://www.cdc.gov/brfss/annual_data/2024/zip/codebook24_llcp-v2-508.zip\"\n",
    "\n",
    "# Define columns to extract using labels from HTML file\n",
    "columns_to_extract = [\n",
    "    \"State FIPS Code\",\n",
    "    \"Annual Sequence Number\",\n",
    "    \"(Ever told) you had diabetes\",\n",
    "    \"Ever been told by a doctor or other health professional that you have pre-diabetes or borderline diabetes?\",\n",
    "    \"What type of diabetes do you have?\",\n",
    "    \"Urban/Rural Status\",\n",
    "    \"Reported age in five-year age categories calculated variable\",\n",
    "    \"Sex of Respondent\",\n",
    "    \"Computed Race-Ethnicity grouping\",\n",
    "    \"Education Level\",\n",
    "    \"Income Level\",\n",
    "    \"Have Personal Health Care Provider?\",\n",
    "    \"Could Not Afford To See Doctor\",\n",
    "    \"Computed Weight in Kilograms\",\n",
    "    \"Computed Height in Meters\",\n",
    "    \"Computed body mass index\",\n",
    "    \"Exercise in Past 30 Days\",\n",
    "    \"How often did you drink regular soda or pop that contains sugar?\",\n",
    "    \"How often did you drink sugar-sweetened drinks?\",\n",
    "    \"Computed Smoking Status\",\n",
    "    \"Computed number of drinks of alcohol beverages per week\",\n",
    "    \"Drink any alcoholic beverages in past 30 days\",\n",
    "    \"Heavy Alcohol Consumption  Calculated Variable\",\n",
    "    \"General Health\",\n",
    "    \"Ever Diagnosed with Heart Attack\",\n",
    "    \"Ever Diagnosed with Angina or Coronary Heart Disease\",\n",
    "    \"Ever Diagnosed with a Stroke\",\n",
    "    \"Ever told you have kidney disease?\",\n",
    "    \"Ever Told Had Asthma\",\n",
    "    \"(Ever told) you had a depressive disorder\",\n",
    "    \"Told Had Arthritis\"\n",
    "]\n",
    "\n",
    "# Output file for writing final dataframe\n",
    "output_file = \"diabetes_data.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498c05de-2843-41e6-b2c3-011afff35339",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08595b6b-db7f-4d6d-a63e-6670659f39c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f4f9b2-7724-424c-8d2c-e2cc63c51244",
   "metadata": {},
   "source": [
    "### Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a10704b-72c0-450a-a18f-23409fe5863b",
   "metadata": {},
   "outputs": [],
   "source": "def parse_brfss_dictionary(html_file):\n    \"\"\"\n    Parse HTML data dictionary to extract both column definitions and value-to-label mappings\n    in a single pass through the file.\n    \n    Parameters:\n    -----------\n    html_file : str\n        Path to the HTML data dictionary file\n    \n    Returns:\n    --------\n    tuple : (column_lookup, codebook)\n        - column_lookup: dict mapping variable labels to metadata\n          Format: {label: {'column_range': str, 'type': str, 'sas_name': str}}\n        - codebook: dict mapping SAS variable names to value-label mappings\n          Format: {sas_variable_name: {value: label}}\n    \"\"\"\n    with open(html_file, 'r', encoding='windows-1252') as f:\n        soup = BeautifulSoup(f, 'html.parser')\n    \n    column_lookup = {}\n    codebook = {}\n    \n    # Find all variable tables (one pass through HTML)\n    tables = soup.find_all('table', {'class': 'table'})\n    \n    for table in tables:\n        # Extract metadata from header cell\n        metadata_cell = table.find('td', {'class': 'l m linecontent'})\n        if not metadata_cell:\n            continue\n        \n        metadata_text = metadata_cell.get_text()\n        \n        # Only process cells that contain variable definitions\n        if 'Label:' not in metadata_text or 'Column:' not in metadata_text:\n            continue\n        \n        # Extract label (between \"Label:\" and \"Section Name:\")\n        # Note: HTML uses \\xa0 (non-breaking spaces)\n        label_match = re.search(r'Label:[\\s\\xa0]+(.+?)Section[\\s\\xa0]+Name:', metadata_text)\n        \n        # Extract column range (format: \"N\" or \"N-M\")\n        column_match = re.search(r'Column:[\\s\\xa0]+(\\d+(?:-\\d+)?)', metadata_text)\n        \n        # Extract variable type (Num or Char)\n        type_match = re.search(r'Type[\\s\\xa0]+of[\\s\\xa0]+Variable:[\\s\\xa0]+(Num|Char)', metadata_text)\n        \n        # Extract SAS variable name (stops before \"Question\")\n        varname_match = re.search(r'SAS[\\s\\xa0]+Variable[\\s\\xa0]+Name:[\\s\\xa0]+(\\w+?)(?=Question)', metadata_text)\n        \n        if not (label_match and column_match and varname_match):\n            continue\n        \n        # Store column metadata\n        label = label_match.group(1).strip().replace('\\xa0', ' ')\n        column_range = column_match.group(1)\n        var_type = type_match.group(1) if type_match else None\n        var_name = varname_match.group(1)\n        \n        column_lookup[label] = {\n            'column_range': column_range,\n            'type': var_type,\n            'sas_name': var_name\n        }\n        \n        # Calculate column width for this variable\n        # This determines the zero-padding needed for values\n        if '-' in column_range:\n            start, end = map(int, column_range.split('-'))\n            column_width = end - start + 1\n        else:\n            column_width = 1\n        \n        # Extract value-label mappings from table body\n        tbody = table.find('tbody')\n        if not tbody:\n            continue\n        \n        value_labels = {}\n        has_categorical_values = False\n        has_range_values = False  # Track if this variable has any range values\n        \n        for row in tbody.find_all('tr'):\n            cells = row.find_all('td')\n            if len(cells) < 2:\n                continue\n            \n            # Extract value (first column)\n            value = cells[0].get_text(strip=True)\n            \n            # Check if this is a range value (e.g., \"1 - 97\", \"100 - 999\")\n            # Range values indicate continuous/numeric variables\n            if ' - ' in value:\n                has_range_values = True\n                continue  # Skip adding ranges to the codebook\n            \n            # Extract label (second column)\n            label_html = cells[1]\n            \n            # Get text and split by line breaks to separate notes\n            label_text = label_html.get_text(separator='|')\n            label_parts = label_text.split('|')\n            \n            # Take first part (before notes)\n            value_label = label_parts[0].strip()\n            \n            # Clean skip logic (remove \"→Go to...\" instructions)\n            value_label = re.sub(r'→Go to.*$', '', value_label).strip()\n            \n            # Clean encoding issues: Replace \"Donï¿½t know\" and similar patterns with \"Unknown\"\n            # This handles character encoding issues from the HTML\n            if re.match(r'^Don.{1,3}t know', value_label, re.IGNORECASE):\n                value_label = 'Unknown'\n            # Also catch \"Refused\" variations and other missing data indicators\n            elif value_label.lower().startswith('refused'):\n                value_label = 'Refused'\n            \n            # Remove encoding error characters and everything after them (e.g., \"Noï¿½Go\" → \"No\")\n            # The pattern ï¿½ is a common Unicode replacement character for malformed text\n            value_label = re.sub(r'ï¿½.*$', '', value_label).strip()\n            \n            # Check if this is a categorical value (not just special codes)\n            # Special codes: HIDDEN, BLANK, and codes like 7/9/77/99/777/999\n            is_special = value in ['HIDDEN', 'BLANK'] or re.match(r'^[79]+$', value)\n            if not is_special:\n                has_categorical_values = True\n            \n            # Pad numeric values to match the column width in the ASCII file\n            # This ensures codebook values match the fixed-width format\n            if value.isdigit() and len(value) < column_width:\n                value_padded = value.zfill(column_width)\n            else:\n                value_padded = value\n            \n            # Store mapping with properly padded value\n            value_labels[value_padded] = value_label\n        \n        # Only add to codebook if:\n        # 1. Has meaningful categorical values (not just HIDDEN/BLANK/special codes)\n        # 2. Does NOT have any range values (which indicate continuous variables)\n        # Variables with ranges (like _DRNKWK3, SSBSUGR2) should remain numeric\n        if value_labels and has_categorical_values and not has_range_values:\n            codebook[var_name] = value_labels\n    \n    print(f\"Successfully parsed {len(column_lookup)} variable definitions and {len(codebook)} value label mappings from HTML dictionary\")\n    return column_lookup, codebook\n\n\ndef apply_value_labels(df, codebook, columns_to_label=None):\n    \"\"\"\n    Apply value-to-label mappings to DataFrame columns.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame with numeric/coded values\n    codebook : dict\n        Value label mappings from parse_brfss_dictionary()\n    columns_to_label : list, optional\n        Specific columns to label. If None, attempts to label all columns.\n    \n    Returns:\n    --------\n    pd.DataFrame : DataFrame with values replaced by labels\n    \"\"\"\n    df_labeled = df.copy()\n    \n    if columns_to_label is None:\n        columns_to_label = df.columns\n    \n    labeled_count = 0\n    skipped_vars = []\n    \n    for col in columns_to_label:\n        # Check if column has value labels in codebook\n        if col not in codebook:\n            continue\n        \n        value_map = codebook[col]\n        \n        # Test mapping on a sample to see if it's appropriate\n        # If less than 50% of non-null values can be mapped, skip this variable\n        # (it's likely a continuous variable or identifier)\n        sample = df[col].dropna().head(1000)\n        if len(sample) > 0:\n            test_mapped = sample.astype(str).str.strip().map(value_map)\n            mapping_success_rate = test_mapped.notna().sum() / len(sample)\n            \n            if mapping_success_rate < 0.5:\n                skipped_vars.append(f\"{col} ({mapping_success_rate:.1%} mappable)\")\n                continue\n        \n        # Apply mapping\n        df_labeled[col] = df[col].astype(str).str.strip().map(value_map)\n        \n        # Count how many values were successfully mapped\n        mapped = df_labeled[col].notna().sum()\n        if mapped > 0:\n            labeled_count += 1\n            print(f\"  Labeled {col}: {mapped:,} / {df[col].notna().sum():,} values ({mapped/df[col].notna().sum()*100:.1f}%)\")\n    \n    if skipped_vars:\n        print(f\"\\n  Skipped (continuous/identifier): {', '.join(skipped_vars)}\")\n    \n    print(f\"\\nSuccessfully labeled {labeled_count} columns\")\n    return df_labeled\n\n\ndef parse_data_file(html_file, asc_file, columns_to_extract):\n    \"\"\"\n    Parse BRFSS ASCII data file using HTML data dictionary.\n    \n    Parameters:\n    -----------\n    html_file : str\n        Path to HTML data dictionary file\n    asc_file : str\n        Path to ASCII data file\n    columns_to_extract : list\n        List of variable labels to extract from the data\n    \n    Returns:\n    --------\n    pd.DataFrame : Parsed data with value labels applied\n    \"\"\"\n    # Step 1: Parse HTML data dictionary (single pass for both metadata and value labels)\n    print(\"Parsing HTML data dictionary...\")\n    column_lookup, codebook = parse_brfss_dictionary(html_file)\n    \n    # Step 2: Convert labels to colspecs for pd.read_fwf()\n    colspecs = []\n    column_names = []\n    dtypes = {}\n    \n    print(\"\\nMapping columns:\")\n    for label in columns_to_extract:\n        if label in column_lookup:\n            col_info = column_lookup[label]\n            col_range = col_info['column_range']\n            \n            # Parse \"1-2\" or \"149\" format\n            if '-' in col_range:\n                start, end = map(int, col_range.split('-'))\n            else:\n                start = end = int(col_range)\n            \n            # Convert to 0-based indexing for Python\n            colspecs.append((start - 1, end))\n            \n            # Use SAS variable name for column name (to match codebook keys)\n            col_name = col_info['sas_name']\n            column_names.append(col_name)\n            \n            # Set dtype (start with string for safety, can convert later)\n            dtypes[col_name] = str\n            \n            print(f\"  {label} -> {col_name} (columns {col_range})\")\n        else:\n            print(f\"  WARNING: '{label}' not found in data dictionary\")\n    \n    print(f\"\\nPrepared to extract {len(colspecs)} columns from ASCII file\")\n    \n    # Step 3: Read the ASCII file using pd.read_fwf()\n    print(f\"\\nReading ASCII file: {asc_file}\")\n    df = pd.read_fwf(\n        asc_file, \n        colspecs=colspecs,\n        names=column_names,\n        dtype=dtypes,\n        encoding='ascii'\n    )\n    \n    print(f\"Successfully loaded {len(df):,} rows and {len(df.columns)} columns\")\n    \n    # Step 4: Apply value labels to DataFrame\n    print(\"\\nApplying value labels to DataFrame...\")\n    df = apply_value_labels(df, codebook)\n    return df\n\n\ndef apply_decimal_transformations(df):\n    \"\"\"\n    Apply decimal transformations to numeric columns with implied decimal places.\n    \n    Transforms columns by dividing by 10^decimal_places to convert from\n    fixed-width integer representation to proper decimal values.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame with columns requiring decimal transformations\n    \n    Returns:\n    --------\n    pd.DataFrame : Transformed DataFrame with proper decimal values\n    \"\"\"\n    # Create a copy for safe rerunning\n    df_transformed = df.copy()\n    \n    # Define columns with implied decimal places (all have 2 decimal places per data dictionary)\n    decimal_transforms = {\n        'WTKG3': {\n            'name': 'Weight (kg)',\n            'decimal_places': 2,\n            'description': 'Computed Weight in Kilograms'\n        },\n        'HTM4': {\n            'name': 'Height (m)',\n            'decimal_places': 2,\n            'description': 'Computed Height in Meters'\n        },\n        '_BMI5': {\n            'name': 'BMI',\n            'decimal_places': 2,\n            'description': 'Computed Body Mass Index'\n        },\n        '_DRNKWK3': {\n            'name': 'Drinks/week',\n            'decimal_places': 2,\n            'description': 'Computed number of drinks per week'\n        }\n    }\n    \n    print(\"Applying decimal place transformations:\\n\")\n    \n    for col, config in decimal_transforms.items():\n        if col in df_transformed.columns:\n            # Convert to numeric, coercing non-numeric values to NaN\n            numeric_vals = pd.to_numeric(df_transformed[col], errors='coerce')\n            \n            # Apply decimal transformation (divide by 10^decimal_places)\n            divisor = 10 ** config['decimal_places']\n            df_transformed[col] = numeric_vals / divisor\n            \n            # Report transformation\n            non_null = df_transformed[col].notna().sum()\n            print(f\"{config['description']} ({col}):\")\n            print(f\"  Transformed {non_null:,} values (÷{divisor})\")\n            print()\n    \n    print(\"✓ Decimal transformations complete\")\n    return df_transformed\n\n\ndef clean_special_codes(df):\n    \"\"\"\n    Clean special codes and outliers in numeric variables.\n    \n    Handles special missing codes and unrealistic outliers in the _DRNKWK3\n    (drinks per week) column.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame with columns requiring special code cleaning\n    \n    Returns:\n    --------\n    pd.DataFrame : Cleaned DataFrame with special codes handled\n    \"\"\"\n    df_cleaned = df.copy()\n    \n    # Clean special codes in _DRNKWK3 (Drinks per week)\n    # Note: This runs AFTER decimal transformation, so 99900 is now 999.00\n    if '_DRNKWK3' in df_cleaned.columns:\n        print(\"Cleaning _DRNKWK3 special codes:\\n\")\n        \n        # Get numeric values (already divided by 100 from decimal transformation)\n        drnkwk3_numeric = df_cleaned['_DRNKWK3'].copy()\n        \n        # Count values before cleaning\n        total_count = drnkwk3_numeric.notna().sum()\n        \n        # Identify special codes and value ranges\n        # After ÷100: 99900 → 999.00 (Don't know/Refused/Missing)\n        missing_code = (drnkwk3_numeric == 999.00).sum()\n        zero_drinks = (drnkwk3_numeric == 0.0).sum()  # Did not drink\n        over_100 = ((drnkwk3_numeric > 100) & (drnkwk3_numeric != 999.00)).sum()  # High values to cap\n        valid_range = ((drnkwk3_numeric > 0) & (drnkwk3_numeric <= 100)).sum()  # 0-100 range\n        \n        print(f\"  Values before cleaning:\")\n        print(f\"    Total non-null: {total_count:,}\")\n        print(f\"    Zero (did not drink): {zero_drinks:,}\")\n        print(f\"    Valid range (0.01-100.00): {valid_range:,}\")\n        print(f\"    Over 100 (will cap at 100): {over_100:,}\")\n        print(f\"    Special code 999.00 (missing): {missing_code:,}\")\n        \n        if over_100 > 0:\n            # Show distribution of high values before capping\n            high_vals = drnkwk3_numeric[(drnkwk3_numeric > 100) & (drnkwk3_numeric != 999.00)]\n            print(f\"\\n  Distribution of values over 100 (before capping):\")\n            print(f\"    Min: {high_vals.min():.2f}\")\n            print(f\"    Max: {high_vals.max():.2f}\")\n            print(f\"    Mean: {high_vals.mean():.2f}\")\n            print(f\"    Median: {high_vals.median():.2f}\")\n        \n        # Apply cleaning transformations:\n        # 1. Map 999.00 (special missing code) to NaN\n        df_cleaned.loc[drnkwk3_numeric == 999.00, '_DRNKWK3'] = np.nan\n        \n        # 2. Cap values over 100 at 100 (data quality issue: >14 drinks/day is unreasonable)\n        df_cleaned.loc[(drnkwk3_numeric > 100) & (drnkwk3_numeric != 999.00), '_DRNKWK3'] = 100.0\n        \n        # Report results\n        remaining = df_cleaned['_DRNKWK3'].notna().sum()\n        removed = total_count - remaining\n        \n        print(f\"\\n  Cleaning applied:\")\n        print(f\"    Mapped 999.00 → NaN: {missing_code:,} values\")\n        print(f\"    Capped >100 → 100.0: {over_100:,} values\")\n        print(f\"    Total removed (NaN): {removed:,} ({removed/total_count*100:.2f}%)\")\n        print(f\"    Remaining valid values: {remaining:,}\")\n        \n        if remaining > 0:\n            print(f\"    Range: {df_cleaned['_DRNKWK3'].min():.2f} - {df_cleaned['_DRNKWK3'].max():.2f}\")\n            print(f\"    Mean: {df_cleaned['_DRNKWK3'].mean():.2f} drinks/week\")\n        print()\n    \n    print(\"✓ Special value cleaning complete\")\n    return df_cleaned\n\n\ndef validate_grain(df, key_cols):\n    \"\"\"\n    Validate that the specified key columns uniquely identify each row.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame to validate\n    key_cols : list\n        List of column names that should form a unique key\n    \n    Returns:\n    --------\n    bool : True if key is unique, False otherwise\n    \"\"\"\n    is_unique = len(df) == len(df.drop_duplicates(subset=key_cols))\n    key_str = ' and '.join([f\"`{col}`\" for col in key_cols])\n    print(f\"Every row is uniquely defined by the {key_str} columns: {is_unique}\")\n    return is_unique\n\n\ndef analyze_missing_values(df, threshold=5.0):\n    \"\"\"\n    Analyze missing values across all columns in the DataFrame.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame to analyze\n    threshold : float, optional\n        Percentage threshold for highlighting columns (default: 5.0)\n    \n    Returns:\n    --------\n    pd.DataFrame : Summary of missing values by column\n    \"\"\"\n    # Calculate missing values and percentages\n    missing_df = pd.DataFrame({\n        'Column': df.columns,\n        'Missing_Count': df.isnull().sum(),\n        'Missing_Percent': (df.isnull().sum() / len(df) * 100).round(2)\n    })\n    \n    # Sort by missing count descending\n    missing_df = missing_df.sort_values('Missing_Count', ascending=False)\n    \n    print(\"Missing Value Summary:\")\n    print(f\"Total rows: {len(df):,}\\n\")\n    print(missing_df.to_string(index=False))\n    \n    # Highlight columns with significant missing data\n    high_missing = missing_df[missing_df['Missing_Percent'] > threshold]\n    if len(high_missing) > 0:\n        print(f\"\\n⚠️  Columns with >{threshold}% missing data:\")\n        print(high_missing.to_string(index=False))\n    else:\n        print(f\"\\n✓ No columns have >{threshold}% missing data\")\n    \n    return missing_df\n\n\ndef verify_data_types(df, categorical_cols, numeric_cols):\n    \"\"\"\n    Verify that columns have the expected data types.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame to verify\n    categorical_cols : list\n        List of columns expected to be categorical/string\n    numeric_cols : list\n        List of columns expected to be numeric\n    \n    Returns:\n    --------\n    pd.DataFrame : Summary of data types by column\n    \"\"\"\n    print(\"Data Type Summary:\\n\")\n    print(f\"{'Column':<15} {'Current Type':<15} {'Expected Category':<20}\")\n    print(\"=\" * 50)\n    \n    type_summary = []\n    \n    for col in df.columns:\n        current_type = str(df[col].dtype)\n        if col in categorical_cols:\n            expected = \"Categorical/String\"\n        elif col in numeric_cols:\n            expected = \"Numeric\"\n        else:\n            expected = \"Unknown\"\n        \n        print(f\"{col:<15} {current_type:<15} {expected:<20}\")\n        type_summary.append({\n            'Column': col,\n            'Current_Type': current_type,\n            'Expected_Category': expected\n        })\n    \n    print(f\"\\n✓ Data type summary complete\")\n    print(f\"  Categorical columns: {len(categorical_cols)} (human-readable labels)\")\n    print(f\"  Numeric columns: {len(numeric_cols)} (transformed with proper decimals)\")\n    \n    return pd.DataFrame(type_summary)\n\n\ndef check_numeric_ranges(df, numeric_check_cols):\n    \"\"\"\n    Check that numeric columns have values within expected ranges.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame to check\n    numeric_check_cols : dict\n        Dictionary mapping column names to range specifications\n        Format: {col_name: {'name': str, 'min': float, 'max': float}}\n    \n    Returns:\n    --------\n    pd.DataFrame : Summary of numeric ranges with out-of-range counts\n    \"\"\"\n    print(\"Numeric Variable Range Summary:\\n\")\n    print(f\"{'Variable':<15} {'Description':<20} {'Count':<10} {'Min':<10} {'Max':<10} {'Mean':<10} {'Valid Range':<20}\")\n    print(\"=\" * 105)\n    \n    range_summary = []\n    \n    for col, info in numeric_check_cols.items():\n        if col in df.columns:\n            # Convert to numeric, coercing errors to NaN\n            numeric_vals = pd.to_numeric(df[col], errors='coerce')\n            \n            count = numeric_vals.notna().sum()\n            min_val = numeric_vals.min() if count > 0 else None\n            max_val = numeric_vals.max() if count > 0 else None\n            mean_val = numeric_vals.mean() if count > 0 else None\n            \n            # Format values\n            min_str = f\"{min_val:.2f}\" if min_val is not None else \"N/A\"\n            max_str = f\"{max_val:.2f}\" if max_val is not None else \"N/A\"\n            mean_str = f\"{mean_val:.2f}\" if mean_val is not None else \"N/A\"\n            range_str = f\"{info['min']}-{info['max']}\"\n            \n            print(f\"{col:<15} {info['name']:<20} {count:<10,} {min_str:<10} {max_str:<10} {mean_str:<10} {range_str:<20}\")\n            \n            # Check for out-of-range values\n            out_of_range = 0\n            if min_val is not None and max_val is not None:\n                out_of_range = ((numeric_vals < info['min']) | (numeric_vals > info['max'])).sum()\n                if out_of_range > 0:\n                    print(f\"  ⚠️  {out_of_range:,} values outside expected range\")\n            \n            range_summary.append({\n                'Variable': col,\n                'Count': count,\n                'Min': min_val,\n                'Max': max_val,\n                'Mean': mean_val,\n                'Out_of_Range': out_of_range\n            })\n    \n    print(\"\\n✓ Range check complete\")\n    return pd.DataFrame(range_summary)\n\n\ndef remove_out_of_range_records(df, numeric_check_cols):\n    \"\"\"\n    Remove records where any numeric variable is outside its expected range.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame to clean\n    numeric_check_cols : dict\n        Dictionary mapping column names to range specifications\n        Format: {col_name: {'name': str, 'min': float, 'max': float}}\n    \n    Returns:\n    --------\n    pd.DataFrame : Cleaned DataFrame with out-of-range records removed\n    \"\"\"\n    # Create a copy for safe rerunning\n    df_clean = df.copy()\n    \n    # Count rows before removal\n    rows_before = len(df_clean)\n    \n    # Track which records to remove (combine all out-of-range conditions)\n    records_to_remove = pd.Series([False] * len(df_clean), index=df_clean.index)\n    \n    print(\"Checking numeric variables for out-of-range values:\\n\")\n    \n    for col, info in numeric_check_cols.items():\n        if col in df_clean.columns:\n            # Convert to numeric, coercing errors to NaN\n            numeric_vals = pd.to_numeric(df_clean[col], errors='coerce')\n            \n            # Identify out-of-range values (excluding NaN)\n            out_of_range = ((numeric_vals < info['min']) | (numeric_vals > info['max'])) & numeric_vals.notna()\n            out_of_range_count = out_of_range.sum()\n            \n            if out_of_range_count > 0:\n                print(f\"  {col} ({info['name']}): {out_of_range_count:,} out-of-range values\")\n                print(f\"    Range: {info['min']}-{info['max']}\")\n                \n                # Add to removal mask\n                records_to_remove = records_to_remove | out_of_range\n            else:\n                print(f\"  {col} ({info['name']}): ✓ All values in range\")\n    \n    # Count total unique records to remove\n    total_to_remove = records_to_remove.sum()\n    \n    print(f\"\\nTotal unique records to remove: {total_to_remove:,}\")\n    \n    if total_to_remove > 0:\n        # Show sample of records being removed\n        print(f\"\\nSample of records being removed (first 10):\")\n        sample_cols = ['_STATE', 'SEQNO'] + [col for col in numeric_check_cols.keys() if col in df_clean.columns]\n        print(df_clean[records_to_remove][sample_cols].head(10).to_string(index=False))\n        \n        # Remove the records\n        df_clean = df_clean[~records_to_remove].copy()\n        \n        rows_after = len(df_clean)\n        rows_removed = rows_before - rows_after\n        \n        print(f\"\\n✓ Removed {rows_removed:,} records ({rows_removed/rows_before*100:.3f}%)\")\n        print(f\"  Rows before: {rows_before:,}\")\n        print(f\"  Rows after: {rows_after:,}\")\n    else:\n        print(f\"\\n✓ No out-of-range values found in any numeric column\")\n    \n    return df_clean\n\n\ndef write_data(df, output_file):\n    \"\"\"\n    Write DataFrame to file in specified format.\n    \n    Supports CSV, Parquet, and Pickle formats based on file extension.\n    \n    Parameters:\n    -----------\n    df : pd.DataFrame\n        DataFrame to write\n    output_file : str\n        Output file path (extension determines format)\n    \n    Returns:\n    --------\n    None\n    \"\"\"\n    if not output_file:\n        print(\"⚠️  No output file specified\")\n        print(f\"\\nDataset available in memory:\")\n        print(f\"  Rows: {len(df):,}\")\n        print(f\"  Columns: {len(df.columns)}\")\n        return\n    \n    # Determine file format from extension\n    if output_file.endswith('.csv'):\n        df.to_csv(output_file, index=False)\n        print(f\"✓ Saved cleaned dataset to CSV: {output_file}\")\n    elif output_file.endswith('.parquet'):\n        df.to_parquet(output_file, index=False)\n        print(f\"✓ Saved cleaned dataset to Parquet: {output_file}\")\n    elif output_file.endswith('.pkl') or output_file.endswith('.pickle'):\n        df.to_pickle(output_file)\n        print(f\"✓ Saved cleaned dataset to Pickle: {output_file}\")\n    else:\n        print(f\"⚠️  Unsupported file format: {output_file}\")\n        print(f\"   Supported formats: .csv, .parquet, .pkl, .pickle\")\n        return\n    \n    # Print summary\n    print(f\"\\nDataset summary:\")\n    print(f\"  Rows: {len(df):,}\")\n    print(f\"  Columns: {len(df.columns)}\")\n    print(f\"  Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")"
  },
  {
   "cell_type": "markdown",
   "id": "5a632e3c-d9bb-4dd9-a2b6-72271645d7c1",
   "metadata": {},
   "source": [
    "## Download data\n",
    "### Raw data file (ASCII)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a75a030-788e-49dd-a352-ea74a7c33529",
   "metadata": {},
   "outputs": [],
   "source": [
    "asc_zip = !ls LLCP2024ASC*.zip\n",
    "# If ascii zip file not present, download it and get zip file name\n",
    "if not asc_zip:\n",
    "    !wget {raw_data_url}\n",
    "    asc_zip = !ls LLCP2024ASC*.zip\n",
    "\n",
    "asc_file = !ls LLCP2024*.ASC*\n",
    "# If raw ascii file not present, unzip ascii zip file and get raw ascii file name\n",
    "if not asc_file:\n",
    "    !unzip {asc_zip[0]}\n",
    "    asc_file = !ls LLCP2024*.ASC*\n",
    "asc_file = asc_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a570fb23-3d16-4b9c-afd4-9c9517123cab",
   "metadata": {},
   "source": [
    "### Data dictionary file (HTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e089197-fe87-4566-bdf4-ad67ce8aa472",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_zip = !ls codebook24_llcp*.zip\n",
    "# If html zip file not present, download it and get html zip file name\n",
    "if not dict_zip:\n",
    "    !wget {data_dict_url}\n",
    "    dict_zip = !ls codebook24_llcp*.zip\n",
    "\n",
    "html_file = !ls USCODE24_LLCP*.HTML\n",
    "# If raw html file not present, unzip html zip file and get raw html file name\n",
    "if not html_file:\n",
    "    !unzip {dict_zip[0]}\n",
    "    html_file = !ls USCODE24_LLCP*.HTML\n",
    "html_file = html_file[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe75d619-f5c0-4ef9-9d88-8d70109aa5a6",
   "metadata": {},
   "source": [
    "## Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bd8c67-e17f-4bd3-adaa-73edc04af27a",
   "metadata": {},
   "outputs": [],
   "source": "data_df = parse_data_file(html_file, asc_file, columns_to_extract)"
  },
  {
   "cell_type": "markdown",
   "id": "gz32r5f9tal",
   "metadata": {},
   "source": [
    "## Transform\n",
    "Apply transformations to convert raw values to their proper formats with correct decimal places and units."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "p70as2qwsu",
   "metadata": {},
   "source": "### Decimal Transformations\nApply decimal place transformations to numeric variables with implied decimal places."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h90xj5qdzb7",
   "metadata": {},
   "outputs": [],
   "source": "transform_df = apply_decimal_transformations(data_df)"
  },
  {
   "cell_type": "markdown",
   "id": "vgy6uvv54qm",
   "metadata": {},
   "source": "### Special Code Cleaning\nClean special codes and outliers in numeric variables."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o9msfi87gdk",
   "metadata": {},
   "outputs": [],
   "source": "transform_df = clean_special_codes(transform_df)"
  },
  {
   "cell_type": "markdown",
   "id": "48c1cc0a-e57c-412a-b6d0-5a932b63b0b2",
   "metadata": {},
   "source": [
    "## Validation\n",
    "### Grain Check\n",
    "Confirm that the `_STATE` and `SEQNO` columns uniquely describe each row of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8839d1e1-136c-4955-bc8a-b905f90d8ee2",
   "metadata": {},
   "outputs": [],
   "source": "validate_grain(transform_df, ['_STATE', 'SEQNO'])"
  },
  {
   "cell_type": "markdown",
   "id": "etpn1suomv",
   "metadata": {},
   "source": [
    "### Missing Value Analysis\n",
    "Check for missing values across all columns to identify data completeness issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l0ecahu8pd",
   "metadata": {},
   "outputs": [],
   "source": "analyze_missing_values(transform_df)"
  },
  {
   "cell_type": "markdown",
   "id": "sdtppagkuo",
   "metadata": {},
   "source": [
    "### Data Type Verification\n",
    "Verify that columns have the expected data types after parsing and value label mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5nr81q4ybmq",
   "metadata": {},
   "outputs": [],
   "source": "# Define expected types for different column categories\ncategorical_cols = ['_STATE', 'DIABETE4', 'PREDIAB2', 'DIABTYPE', '_URBSTAT', '_AGEG5YR', \n                    'SEXVAR', '_RACE', 'EDUCA', 'INCOME3', 'PERSDOC3', 'MEDCOST1', \n                    'EXERANY2', '_SMOKER3', 'DRNKANY6', '_RFDRHV9', 'GENHLTH', 'CVDINFR4',\n                    'CVDCRHD4', 'CVDSTRK3', 'CHCKDNY2', 'ASTHMA3', 'ADDEPEV3', 'HAVARTH4']\n\nnumeric_cols = ['SEQNO', 'WTKG3', 'HTM4', '_BMI5', 'SSBSUGR2', 'SSBFRUT3', '_DRNKWK3']\n\nverify_data_types(transform_df, categorical_cols, numeric_cols)"
  },
  {
   "cell_type": "markdown",
   "id": "t9vsqf38lx",
   "metadata": {},
   "source": [
    "### Range Checks for Numeric Variables\n",
    "Verify that numeric columns have reasonable values within expected ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ls0anfr18",
   "metadata": {},
   "outputs": [],
   "source": "# Define numeric columns to check\nnumeric_check_cols = {\n    'WTKG3': {'name': 'Weight (kg)', 'min': 23.00, 'max': 295.00},\n    'HTM4': {'name': 'Height (m)', 'min': 0.91, 'max': 2.44},\n    '_BMI5': {'name': 'BMI', 'min': 0.01, 'max': 99.99},\n    'SSBSUGR2': {'name': 'Sugar soda freq', 'min': 0, 'max': 999},\n    'SSBFRUT3': {'name': 'Sugar drink freq', 'min': 0, 'max': 999},\n    '_DRNKWK3': {'name': 'Drinks/week', 'min': 0.0, 'max': 100.0}  # After ÷100 and capping at 100\n}\n\ncheck_numeric_ranges(transform_df, numeric_check_cols)"
  },
  {
   "cell_type": "markdown",
   "id": "plrr1muw3hg",
   "metadata": {},
   "source": [
    "## Data Removal\n",
    "Remove records with data quality issues identified during validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upe3hflpb4",
   "metadata": {},
   "source": [
    "### Remove Out-of-Range Numeric Records\n",
    "Remove records where any numeric variable is outside its expected range as specified in the data dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hcwdsvk36sb",
   "metadata": {},
   "outputs": [],
   "source": "# Use the same range definitions from the validation section\nnumeric_check_cols = {\n    'WTKG3': {'name': 'Weight (kg)', 'min': 23.00, 'max': 295.00},\n    'HTM4': {'name': 'Height (m)', 'min': 0.91, 'max': 2.44},\n    '_BMI5': {'name': 'BMI', 'min': 0.01, 'max': 99.99},\n    'SSBSUGR2': {'name': 'Sugar soda freq', 'min': 0, 'max': 999},\n    'SSBFRUT3': {'name': 'Sugar drink freq', 'min': 0, 'max': 999},\n    '_DRNKWK3': {'name': 'Drinks/week', 'min': 0.0, 'max': 100.0}  # After ÷100 and capping at 100\n}\n\nclean_df = remove_out_of_range_records(transform_df, numeric_check_cols)"
  },
  {
   "cell_type": "markdown",
   "id": "ouftzhqu89",
   "metadata": {},
   "source": [
    "## Write Data\n",
    "Save the cleaned and transformed dataset to file for use in EDA and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ei93mxyug",
   "metadata": {},
   "outputs": [],
   "source": "write_data(clean_df, output_file)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915db5f6-61d8-4b3d-b57d-4523fbbc09c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}